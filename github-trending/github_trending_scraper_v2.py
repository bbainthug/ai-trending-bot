#!/usr/bin/env python3
"""
GitHub Trending Scraper
æå–GitHub Trendingé¡µé¢çš„ä»“åº“ä¿¡æ¯å¹¶ä¿å­˜ä¸ºMarkdownè¡¨æ ¼
"""

import requests
from bs4 import BeautifulSoup
import re


def scrape_github_trending():
    """
    ä»GitHub Trendingé¡µé¢æå–ä»“åº“ä¿¡æ¯
    
    Returns:
        list: åŒ…å«ä»“åº“ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    url = "https://github.com/trending"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âŒ è·å–é¡µé¢å¤±è´¥: {e}")
        return []
    
    soup = BeautifulSoup(response.content, "html.parser")
    repo_elements = soup.find_all("article", class_="Box-row")
    
    if not repo_elements:
        print("âš ï¸ æœªæ‰¾åˆ°ä»“åº“å…ƒç´ ï¼Œé¡µé¢ç»“æ„å¯èƒ½å·²æ›´æ”¹")
        return []
    
    trending_repos = []
    
    for repo in repo_elements:
        repo_info = extract_repo_info(repo)
        if repo_info:
            trending_repos.append(repo_info)
    
    return trending_repos


def extract_repo_info(repo_element):
    """
    ä»å•ä¸ªä»“åº“å…ƒç´ ä¸­æå–ä¿¡æ¯
    
    Args:
        repo_element: BeautifulSoupä»“åº“å…ƒç´ 
        
    Returns:
        dict: åŒ…å«ä»“åº“ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å›None
    """
    try:
        # æå–ä»“åº“åç§°å’ŒURL
        h2_tag = repo_element.find("h2", class_="h3")
        if not h2_tag:
            return None
            
        a_tag = h2_tag.find("a")
        if not a_tag:
            return None
            
        repo_name = a_tag.get_text(strip=True).replace(" ", "")
        repo_url = "https://github.com" + a_tag["href"]
        
        # éªŒè¯URLæ ¼å¼
        if not re.match(r'^https://github\.com/[^/]+/[^/]+$', repo_url):
            return None
        
        # æå–æè¿°
        description = "N/A"
        p_tag = repo_element.find("p", class_="col-9")
        if p_tag:
            description = p_tag.get_text(strip=True)
        
        # æå–æ˜Ÿæ•°
        stars = "0"
        star_link = repo_element.find("a", href=lambda x: x and "/stargazers" in x)
        if star_link:
            stars_text = star_link.get_text(strip=True)
            # ç§»é™¤é€—å·å¹¶éªŒè¯æ˜¯å¦ä¸ºæ•°å­—
            stars = stars_text.replace(",", "")
            if not stars.isdigit():
                stars = "0"
        
        # æå–ç¼–ç¨‹è¯­è¨€ï¼ˆå¯é€‰ï¼‰
        language = "N/A"
        lang_span = repo_element.find("span", itemprop="programmingLanguage")
        if lang_span:
            language = lang_span.get_text(strip=True)
        
        return {
            "name": repo_name,
            "url": repo_url,
            "description": description,
            "stars": stars,
            "language": language
        }
        
    except Exception as e:
        print(f"âš ï¸ æå–ä»“åº“ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return None


def filter_ai_repos(repos):
    """
    è¿‡æ»¤AI/LLM/Agentç›¸å…³çš„ä»“åº“
    
    Args:
        repos: ä»“åº“åˆ—è¡¨
        
    Returns:
        list: è¿‡æ»¤åçš„ä»“åº“åˆ—è¡¨
    """
    if not repos:
        return []
    
    ai_keywords = ["ai", "llm", "agent", "artificial intelligence", 
                   "machine learning", "deep learning", "neural network",
                   "transformer", "gpt", "chatgpt", "openai", "anthropic",
                   "claude", "gemini", "vector", "embedding", "rag"]
    
    filtered_repos = []
    
    for repo in repos:
        description_lower = repo["description"].lower()
        name_lower = repo["name"].lower()
        
        # æ£€æŸ¥æè¿°æˆ–åç§°ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        for keyword in ai_keywords:
            if keyword in description_lower or keyword in name_lower:
                filtered_repos.append(repo)
                break
    
    return filtered_repos


def create_markdown_table(repos):
    """
    åˆ›å»ºMarkdownè¡¨æ ¼
    
    Args:
        repos: ä»“åº“åˆ—è¡¨
        
    Returns:
        str: Markdownæ ¼å¼çš„è¡¨æ ¼
    """
    if not repos:
        return "# GitHub Trending (AI/LLM/Agentç›¸å…³)\n\næœªæ‰¾åˆ°ç›¸å…³ä»“åº“ã€‚"
    
    # è¡¨æ ¼æ ‡é¢˜
    markdown = "# GitHub Trending (AI/LLM/Agentç›¸å…³)\n\n"
    markdown += "| ä»“åº“åç§° | URL | æè¿°ï¼ˆåŠŸèƒ½ï¼‰ | æ˜Ÿæ•° |\n"
    markdown += "|----------|-----|--------------|------|\n"
    
    # è¡¨æ ¼å†…å®¹
    for repo in repos:
        # è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦
        name = repo["name"].replace("|", "\\|")
        url = repo["url"]
        description = repo["description"].replace("|", "\\|").replace("\n", " ")
        stars = repo["stars"]
        
        markdown += f"| {name} | [{url}]({url}) | {description} | {stars} |\n"
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    markdown += f"\n**æ€»è®¡: {len(repos)} ä¸ªä»“åº“**\n"
    
    return markdown


def save_to_file(content, filename="trending_today.md"):
    """
    ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
    
    Args:
        content: è¦ä¿å­˜çš„å†…å®¹
        filename: æ–‡ä»¶å
    """
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {filename}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æŠ“å–GitHub Trendingé¡µé¢...")
    
    # æŠ“å–æ‰€æœ‰ä»“åº“
    all_repos = scrape_github_trending()
    
    if not all_repos:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•ä»“åº“")
        return
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(all_repos)} ä¸ªä»“åº“")
    
    # è¿‡æ»¤AIç›¸å…³ä»“åº“
    ai_repos = filter_ai_repos(all_repos)
    
    if not ai_repos:
        print("âš ï¸ æœªæ‰¾åˆ°AI/LLM/Agentç›¸å…³çš„ä»“åº“")
        # æ˜¾ç¤ºå‰5ä¸ªä»“åº“ä¾›å‚è€ƒ
        print("\nå‰5ä¸ªçƒ­é—¨ä»“åº“:")
        for i, repo in enumerate(all_repos[:5], 1):
            print(f"{i}. {repo['name']} - {repo['description'][:50]}...")
        return
    
    print(f"ğŸ¤– æ‰¾åˆ° {len(ai_repos)} ä¸ªAI/LLM/Agentç›¸å…³ä»“åº“")
    
    # åˆ›å»ºMarkdownè¡¨æ ¼
    markdown_content = create_markdown_table(ai_repos)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    if save_to_file(markdown_content):
        # æ˜¾ç¤ºå‰å‡ ä¸ªä»“åº“
        print("\nğŸ“‹ å‰å‡ ä¸ªAIç›¸å…³ä»“åº“:")
        for i, repo in enumerate(ai_repos[:3], 1):
            print(f"{i}. {repo['name']}")
            print(f"   æè¿°: {repo['description'][:60]}...")
            print(f"   æ˜Ÿæ•°: {repo['stars']}")
            print(f"   URL: {repo['url']}")
            print()
        
        if len(ai_repos) > 3:
            print(f"... è¿˜æœ‰ {len(ai_repos) - 3} ä¸ªä»“åº“")
    else:
        print("âŒ ä¿å­˜å¤±è´¥")


if __name__ == "__main__":
    main()